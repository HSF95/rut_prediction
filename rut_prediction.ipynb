{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4ff5da",
   "metadata": {},
   "source": [
    "# 整体配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49755c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8353922",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d6a815",
   "metadata": {},
   "source": [
    "# 车辙数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1e3a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全局配置\n",
    "RUTPATH = '数据大赛/性能数据/车辙数据'\n",
    "COLUMNS_NAME = [ '日期', '轮迹带1均值', '轮迹带1标准差', '轮迹带1变异系数', '轮迹带1样本量', \n",
    "                   '轮迹带2均值', '轮迹带2标准差', '轮迹带2变异系数', '轮迹带2样本量',\n",
    "                   '断面均值', '断面标准差', '断面变异系数', '断面样本量']\n",
    "KEY_WORDS = ['STR2', 'STR8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 从文件名中获取日期\n",
    "def extract_and_convert_date(filename):\n",
    "    match = re.search(r'(\\d{8})\\.xlsx$', filename)\n",
    "    if match:\n",
    "        date_str = match.group(1)  # 提取匹配到的日期字符串\n",
    "        date_obj = datetime.strptime(date_str, '%Y%m%d')\n",
    "        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "        return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd4d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 基于key_word提取对应的行数\n",
    "def get_rut_data_rows(df, key_word):\n",
    "    result_rows = []\n",
    "    start_index = None\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i, 0] == key_word:  # 找到key_word所在行\n",
    "            start_index = i\n",
    "            result_rows.append(i)  # 添加这个行序\n",
    "        elif start_index is not None and pd.isna(df.iloc[i, 0]):  # 开始行下面的空白行\n",
    "            result_rows.append(i)  # 添加这个行序\n",
    "        elif start_index is not None and not pd.isna(df.iloc[i, 0]):  # 直到下一次遇到非空行\n",
    "            break\n",
    "    return result_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb5d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 找到满足“第*次”的列\n",
    "def get_rut_data_columns(df):\n",
    "    result_columns = []\n",
    "    str_list = df.iloc[0, :].values  # 提取表头\n",
    "    pattern = r'^第.+次$'\n",
    "    for i in range(len(str_list)):\n",
    "        if not pd.isna(str_list[i]):\n",
    "            if bool(re.fullmatch(pattern, str(str_list[i]))):\n",
    "                result_columns.append(i)\n",
    "                result_columns.append(i+1)\n",
    "    return result_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4486bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 对target_arr作统计分析\n",
    "# target_arr是一个具有偶数列的array，所有奇数列都是轮迹带1所有偶数列都是轮迹带2，要分别求均值、标准差和变异系数，还有整个断面的\n",
    "def summary_target_arr(target_arr, rut_date):\n",
    "    # 两列两列地拼在一起，例如(7,6)的数组会被改成(21,2)\n",
    "    rows_num, columns_num = target_arr.shape\n",
    "    pairs_num = int(columns_num / 2)\n",
    "    target_arr_reshape = np.zeros((rows_num * pairs_num, 2))\n",
    "    for i in range(pairs_num):\n",
    "        row_start = i * rows_num\n",
    "        row_end = (i + 1) * rows_num\n",
    "        target_arr_reshape[row_start:row_end, :] = target_arr[:, (i * 2):(i * 2 + 2)]\n",
    "        \n",
    "    # 开始整理统计量\n",
    "    rut_data_tmp = pd.DataFrame(columns = COLUMNS_NAME)\n",
    "    columns_mean = np.average(target_arr_reshape, axis=0)\n",
    "    columns_std = np.std(target_arr_reshape, axis=0)\n",
    "    columns_cv = columns_std / columns_mean\n",
    "    rut_data_tmp.loc[0, '日期'] = rut_date\n",
    "    rut_data_tmp.loc[0, '轮迹带1均值'], rut_data_tmp.loc[0, '轮迹带2均值'] = columns_mean\n",
    "    rut_data_tmp.loc[0, '轮迹带1标准差'], rut_data_tmp.loc[0, '轮迹带2标准差'] = columns_std\n",
    "    rut_data_tmp.loc[0, '轮迹带1变异系数'], rut_data_tmp.loc[0, '轮迹带2变异系数'] = columns_cv\n",
    "    rut_data_tmp.loc[0, '轮迹带1样本量'] = rows_num * pairs_num\n",
    "    rut_data_tmp.loc[0, '轮迹带2样本量'] = rows_num * pairs_num\n",
    "    rut_data_tmp.loc[0, '断面均值'] = np.mean(target_arr_reshape)\n",
    "    rut_data_tmp.loc[0, '断面标准差'] = np.std(target_arr_reshape)\n",
    "    rut_data_tmp.loc[0, '断面变异系数'] = np.std(target_arr_reshape) / np.mean(target_arr_reshape)\n",
    "    rut_data_tmp.loc[0, '断面样本量'] = rows_num * columns_num\n",
    "    \n",
    "    return rut_data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6fcbc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取车辙数据\n",
    "def get_rut_data(key_word):\n",
    "    # keyword分别是STR2和STR8\n",
    "    rut_data = pd.DataFrame(columns = COLUMNS_NAME)  # 空白df，逐一存放rut_data_tmp\n",
    "    folder_list = os.listdir(RUTPATH)  # 搜索文件夹中所有文件夹\n",
    "    for folder in folder_list:\n",
    "        print(f'正在读取{folder}')\n",
    "        file_list = os.listdir(str(RUTPATH) + '/' + folder)  # 搜索子文件夹中所有文件\n",
    "        for file_name in file_list:\n",
    "            print(f'正在读取{file_name}')\n",
    "            rut_date = extract_and_convert_date(file_name)  # 从文件名中获取日期\n",
    "            file_path = str(RUTPATH) + '/' + folder + '/' + file_name  # 完整相对路径\n",
    "            \n",
    "            with pd.ExcelFile(file_path) as xls:  # 判断是否有个表叫“内侧车道”\n",
    "                if '内侧车道' in xls.sheet_names:\n",
    "                    sheet_name = '内侧车道'\n",
    "                else:\n",
    "                    sheet_name = 'Sheet1'\n",
    "            \n",
    "            # 提取完整的数据，例如STR2的所有次数的轮迹带1和2的所有桩号数据\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "            rut_data_rows = get_rut_data_rows(df, key_word)  # 基于key_word提取对应的行数\n",
    "            rut_data_columns = get_rut_data_columns(df)  # 找到满足“第*次”的列\n",
    "            target_arr = df.iloc[rut_data_rows, rut_data_columns].to_numpy(dtype=float)  # 这是完整的array数据\n",
    "            rut_data_tmp = summary_target_arr(target_arr, rut_date)\n",
    "            rut_data = pd.concat([rut_data, rut_data_tmp], ignore_index=True)\n",
    "    return rut_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159930a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算并汇总\n",
    "key_word = KEY_WORDS[1]\n",
    "rut_data = get_rut_data(key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f82e11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看并输出\n",
    "print(rut_data)\n",
    "csv_name = key_word + '车辙数据.csv'\n",
    "rut_data.to_csv(csv_name, index=False)  # index=False 表示不保存索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9029a",
   "metadata": {},
   "source": [
    "# 温度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e648c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全局配置\n",
    "TEPPATH = '数据大赛/环境数据/路面结构温湿度数据'\n",
    "KEY_WORDS = ['STR2', 'STR8']\n",
    "SENSOR_NUMS = [9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55062a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tep_data(key_word_index):\n",
    "    \n",
    "    # 列名\n",
    "    sensor_index_tmp = np.arange(SENSOR_NUMS[key_word_index]) + 1\n",
    "    sensor_index = np.char.add('W', sensor_index_tmp.astype(str))\n",
    "    columns_title = np.insert(sensor_index, 0, 'timestamp')\n",
    "    \n",
    "    # 读取数据\n",
    "    tepdata = pd.DataFrame(columns = columns_title)  # 空数组\n",
    "    folder_list = os.listdir(TEPPATH)\n",
    "    for folder in folder_list:\n",
    "        print(f'正在读取{folder}')\n",
    "        file_list = os.listdir(str(TEPPATH) + '/' + folder)\n",
    "        for file_name in file_list:\n",
    "            if bool(re.search(KEY_WORDS[key_word_index], file_name)):  # 找到文件名带有STR2或8的\n",
    "                print(f'正在读取{file_name}')\n",
    "                file_path = str(TEPPATH) + '/' + folder + '/' + file_name  # 完整相对路径\n",
    "                with pd.ExcelFile(file_path) as xls:  # 判断是否有个表叫“汇总”\n",
    "                    if '汇总' in xls.sheet_names:\n",
    "                        sheet_name = '汇总'\n",
    "                        df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "                        df.columns = columns_title\n",
    "                        df = df.loc[5:, :]\n",
    "                        tepdata = pd.concat([tepdata, df], ignore_index=True)\n",
    "                    else:\n",
    "                        for sheet_name in xls.sheet_names:\n",
    "                            print(f'正在读取{sheet_name}')\n",
    "                            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)\n",
    "                            df.columns = columns_title\n",
    "                            df = df.loc[3:, :]\n",
    "                            tepdata = pd.concat([tepdata, df], ignore_index=True)\n",
    "    return tepdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tep_data(tepdata):\n",
    "    \n",
    "    # 定义日期时间格式的正则表达式\n",
    "    # 匹配格式为YYYY-MM-DD HH:MM:SS[.ffffff]\n",
    "    pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}(\\.\\d{6})?$'\n",
    "    \n",
    "    valid_indices = []\n",
    "    # 找出不符合格式的行并删除\n",
    "    for index, row in tepdata.iterrows():\n",
    "        if re.match(pattern, str(row['timestamp'])):\n",
    "            valid_indices.append(index)\n",
    "            # 去掉微秒部分\n",
    "            if '.' in str(row['timestamp']):\n",
    "                cleaned_timestamp = str(row['timestamp']).split('.')[0]\n",
    "                tepdata.loc[index, 'timestamp'] = cleaned_timestamp\n",
    "    # 删除不符合格式的行\n",
    "    tepdata = tepdata.loc[valid_indices].reset_index(drop=True)\n",
    "    return tepdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52b7ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 计算并汇总\n",
    "key_word_index = 0\n",
    "tepdata = get_tep_data(key_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e392fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗\n",
    "tepdata_clear = clear_tep_data(tepdata)\n",
    "print(tepdata_clear)\n",
    "csv_name = KEY_WORDS[key_word_index] + '温度数据.csv'\n",
    "tepdata_clear.to_csv(csv_name, index=False)  # index=False 表示不保存索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a6279",
   "metadata": {},
   "source": [
    "# 应力数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局配置\n",
    "STRESS_PATH = r'E:\\BaiduNetdiskDownload\\STR2+STR8'\n",
    "KEY_WORDS = ['STR2', 'STR8']\n",
    "SENSOR_TYPES = ['SP', 'SB', 'SV', 'PF', 'DM']\n",
    "SENSOR_TYPES_DESCRIBE = ['沥青应变', '混凝土应变', '竖向应变', '土压力', '多点位移']\n",
    "SENSOR_NUMS = [[8, 24, 2, 7, 4], [8, 24, 2, 7, 3]]\n",
    "CHUCK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stress_data(key_word_index, sensor_type_index):\n",
    "    \n",
    "    # 列名\n",
    "    sensor_index_tmp = np.arange(SENSOR_NUMS[key_word_index][sensor_type_index]) + 1\n",
    "    sensor_index = np.char.add('Y', sensor_index_tmp.astype(str))\n",
    "    columns_title = np.insert(sensor_index, 0, 'timestamp')\n",
    "    \n",
    "    # 数据\n",
    "    stress_data = pd.DataFrame(columns = columns_title)\n",
    "    \n",
    "    # 初始化一个last_second\n",
    "    last_second = None\n",
    "    \n",
    "    # 打开文件夹\n",
    "    folder_list = os.listdir(STRESS_PATH)  # 20200319、20200320...20200428\n",
    "    for folder in folder_list:\n",
    "        folder_path = os.path.join(STRESS_PATH, folder, KEY_WORDS[key_word_index])\n",
    "        print(f'正在读取{folder_path}')\n",
    "        if os.path.exists(folder_path):\n",
    "            file_list = os.listdir(folder_path)  # ../20200319/STR2文件夹的文件列表\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for file_name in file_list:\n",
    "            if bool(re.search(SENSOR_TYPES[sensor_type_index], file_name)):  # 找到有传感器类型对应的文件\n",
    "                print(f'正在读取{file_name}')\n",
    "                file_path = os.path.join(STRESS_PATH, folder, KEY_WORDS[key_word_index], file_name)\n",
    "                chunk_iterator = pd.read_csv(file_path, chunksize=CHUCK_SIZE, parse_dates=['DateTime'], \n",
    "                                            on_bad_lines='skip')  # 逐块取数据\n",
    "                \n",
    "                # 每一块\n",
    "                for chunk in chunk_iterator:\n",
    "                    if len(chunk) == 0:\n",
    "                        break\n",
    "                    chunk['DateTime'] = chunk['DateTime'].dt.floor('S')  # 消除微秒\n",
    "                    second_stress_data = chunk.groupby('DateTime').first().reset_index()  # 每一秒取第一行\n",
    "                    second_stress_data.columns = columns_title\n",
    "                    if second_stress_data.iloc[0, 0] == last_second and len(second_stress_data) > 0:\n",
    "                        second_stress_data = second_stress_data.iloc[1:]\n",
    "                    if len(second_stress_data) > 0:\n",
    "                        last_second = second_stress_data.iloc[-1, 0]\n",
    "                    \n",
    "                    stress_data = pd.concat([stress_data, second_stress_data], ignore_index=True)\n",
    "\n",
    "    return stress_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de9e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 汇总\n",
    "key_word_index = 1\n",
    "sensor_type_index = 4\n",
    "stress_data = get_stress_data(key_word_index, sensor_type_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7562327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看\n",
    "print(stress_data.head())\n",
    "print(len(stress_data))\n",
    "print(f\"总内存: {stress_data.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出\n",
    "csv_name = KEY_WORDS[key_word_index] + SENSOR_TYPES_DESCRIBE[sensor_type_index] + '数据.csv'\n",
    "print(csv_name)\n",
    "stress_data.to_csv(csv_name, index=False)  # index=False 表示不保存索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c42ff3",
   "metadata": {},
   "source": [
    "# 轴次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149196d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axle_data(file_path):\n",
    "    # 生成每一天的轴次数据\n",
    "    axle_data = pd.DataFrame(columns=['日期', '边缘轴次'])\n",
    "    axle_df = pd.read_excel(file_path, parse_dates=['开始时间', '结束时间'])\n",
    "    for i in range(len(axle_df)):\n",
    "        date_sequence = pd.date_range(start=axle_df.loc[i, '开始时间'], \n",
    "                                      end=axle_df.loc[i, '结束时间'], freq='D')  # 生成日期序列（包含头尾）\n",
    "        date_num = len(date_sequence)\n",
    "        axle_data_tmp = pd.DataFrame(columns=['日期', '边缘轴次'])\n",
    "        axle_data_tmp['日期'] = date_sequence\n",
    "        daily_axle = axle_df.loc[i, '边际轴次'] / axle_df.loc[i, '时长']\n",
    "        daily_axle_sequence = daily_axle * np.ones(date_num)\n",
    "        axle_data_tmp['边缘轴次'] = daily_axle_sequence\n",
    "        axle_data = pd.concat([axle_data, axle_data_tmp], ignore_index=True)\n",
    "    axle_data['日期'] = pd.to_datetime(axle_data['日期'])\n",
    "    axle_data['日期'] = axle_data['日期'].dt.floor('D')\n",
    "    axle_data['累计轴次'] = axle_data['边缘轴次'].cumsum()\n",
    "    return axle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a556bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理并查看\n",
    "file_path = '轴次数据.xlsx'\n",
    "axle_data = get_axle_data(file_path)\n",
    "print(axle_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出\n",
    "csv_name = '单日轴次数据.csv'\n",
    "axle_data.to_csv(csv_name, index=False)  # index=False 表示不保存索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103609a",
   "metadata": {},
   "source": [
    "# 二次加载+可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9b2d1",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轴次数据\n",
    "axle_data = pd.read_csv('单日轴次数据.csv', parse_dates=['日期'])\n",
    "axle_data = axle_data.sort_values(by='日期')\n",
    "print(axle_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13610c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 车辙数据\n",
    "STR2_rut_data = pd.read_csv('STR2车辙数据.csv', parse_dates=['日期'])\n",
    "STR2_rut_data = STR2_rut_data.sort_values(by='日期')\n",
    "print(STR2_rut_data.head())\n",
    "\n",
    "# 温度数据\n",
    "STR2_tep_data = pd.read_csv('STR2温度数据.csv', parse_dates=['timestamp'], low_memory=False)\n",
    "STR2_tep_data = STR2_tep_data.sort_values(by='timestamp')\n",
    "print(STR2_tep_data.head())\n",
    "\n",
    "# 土压力数据\n",
    "STR2_stress_data = pd.read_csv('STR2土压力数据.csv', parse_dates=['timestamp'])\n",
    "STR2_stress_data = STR2_stress_data.sort_values(by='timestamp')\n",
    "print(STR2_stress_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7403a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 车辙数据\n",
    "STR8_rut_data = pd.read_csv('STR8车辙数据.csv', parse_dates=['日期'])\n",
    "STR8_rut_data = STR8_rut_data.sort_values(by='日期')\n",
    "print(STR8_rut_data.head())\n",
    "\n",
    "# 温度数据\n",
    "STR8_tep_data = pd.read_csv('STR8温度数据.csv', parse_dates=['timestamp'], low_memory=False)\n",
    "STR8_tep_data = STR8_tep_data.sort_values(by='timestamp')\n",
    "print(STR8_tep_data.head())\n",
    "\n",
    "# 土压力数据\n",
    "STR8_stress_data = pd.read_csv('STR8土压力数据.csv', parse_dates=['timestamp'])\n",
    "STR8_stress_data = STR8_stress_data.sort_values(by='timestamp')\n",
    "print(STR8_stress_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01736bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日期统一\n",
    "max_date = axle_data.iloc[-1, 0]  # 取轴次的最大日期为期限\n",
    "\n",
    "# STR2过滤\n",
    "STR2_rut_data = STR2_rut_data[STR2_rut_data['日期'] <= max_date]\n",
    "STR2_tep_data = STR2_tep_data[STR2_tep_data['timestamp'] <= max_date]\n",
    "STR2_stress_data = STR2_stress_data[STR2_stress_data['timestamp'] <= max_date]\n",
    "\n",
    "# STR8过滤\n",
    "STR8_rut_data = STR8_rut_data[STR8_rut_data['日期'] <= max_date]\n",
    "STR8_tep_data = STR8_tep_data[STR8_tep_data['timestamp'] <= max_date]\n",
    "STR8_stress_data = STR8_stress_data[STR8_stress_data['timestamp'] <= max_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e61e0e",
   "metadata": {},
   "source": [
    "### 温度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7459d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 温度格式转换\n",
    "columns_to_convert = STR2_tep_data.columns[1:]\n",
    "STR2_tep_data[columns_to_convert] = STR2_tep_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "columns_to_convert = STR8_tep_data.columns[1:]\n",
    "STR8_tep_data[columns_to_convert] = STR8_tep_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_gap_lengths(mask):\n",
    "    \"\"\"计算布尔掩码中连续True的片段\"\"\"\n",
    "    # 找到掩码中True值的开始和结束位置\n",
    "    mask = mask.values\n",
    "    if not mask.any():\n",
    "        return []\n",
    "    \n",
    "    # 找到变化点\n",
    "    idx = np.flatnonzero(np.diff(np.r_[False, mask, False]))\n",
    "    starts = idx[0::2]\n",
    "    ends = idx[1::2]\n",
    "    \n",
    "    # 计算每个片段的长度\n",
    "    lengths = ends - starts\n",
    "    \n",
    "    return list(zip(starts, ends, lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69adef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 温度插值\n",
    "def interpolate_temperature_data(data, method='cubic_spline', max_gap=10, limit_direction='both'):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        \n",
    "        if column == 'timestamp':\n",
    "            continue\n",
    "        \n",
    "        print(f'正在处理{column}')\n",
    "    \n",
    "        # 确保温度列是数值类型\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "        # 计算空缺值位置\n",
    "        missing_mask = df[column].isna()\n",
    "        missing_count = missing_mask.sum()\n",
    "\n",
    "        print(f\"原始数据共有 {len(df)} 行，其中 {missing_count} 行是空缺值\")\n",
    "\n",
    "        # 根据选择的方法进行插值\n",
    "        if method == 'linear':\n",
    "            # 线性插值\n",
    "            df[column] = df[column].interpolate(\n",
    "                method='linear', \n",
    "                limit=max_gap, \n",
    "                limit_direction=limit_direction\n",
    "            )\n",
    "\n",
    "        elif method == 'quadratic':\n",
    "            # 二次插值（使用pandas的polynomial方法，阶数为2）\n",
    "            df[column] = df[column].interpolate(\n",
    "                method='polynomial', \n",
    "                order=2, \n",
    "                limit=max_gap, \n",
    "                limit_direction=limit_direction\n",
    "            )\n",
    "\n",
    "        elif method == 'cubic_spline':\n",
    "            # 三次样条插值\n",
    "            # 先进行线性插值处理短空缺\n",
    "            df[column] = df[column].interpolate(\n",
    "                method='linear', \n",
    "                limit=max_gap//2,  # 先处理较短的空缺\n",
    "                limit_direction=limit_direction\n",
    "            )\n",
    "\n",
    "            # 对剩余的较长空缺使用三次样条插值\n",
    "            non_missing = df[column].dropna()\n",
    "            x = non_missing.index.astype(np.int64) // 10**9  # 转换为unix时间（秒）\n",
    "            y = non_missing.values\n",
    "\n",
    "            # 创建三次样条函数\n",
    "            if len(non_missing) >= 4:  # 至少需要4个点来构建三次样条\n",
    "                # 转换为unix时间（秒）并确保严格递增\n",
    "                x = non_missing.index.astype(np.int64) // 10**9\n",
    "                y = non_missing.values\n",
    "\n",
    "                # 检查是否严格递增\n",
    "                if np.all(np.diff(x) > 0):\n",
    "                    # 如果是严格递增的，直接使用\n",
    "                    spl = interpolate.UnivariateSpline(x, y, s=0)\n",
    "                else:\n",
    "                    # 如果不是，创建一个严格递增的索引\n",
    "                    x_new = np.arange(len(x))\n",
    "                    spl = interpolate.UnivariateSpline(x_new, y, s=0)\n",
    "                    # 需要将原始x映射到新索引\n",
    "                    x_mapping = {orig_x: new_x for orig_x, new_x in zip(x, x_new)}\n",
    "\n",
    "                # 对剩余的空缺值应用样条插值\n",
    "                remaining_missing = df[column].isna()\n",
    "                missing_indices = df.index[remaining_missing]\n",
    "                missing_x = missing_indices.astype(np.int64) // 10**9\n",
    "\n",
    "                # 只对连续空缺长度小于max_gap的部分进行插值\n",
    "                gaps = _calculate_gap_lengths(remaining_missing)\n",
    "                valid_missing = remaining_missing.copy()\n",
    "\n",
    "                for start, end, length in gaps:\n",
    "                    if length > max_gap:\n",
    "                        valid_missing[start:end] = False\n",
    "\n",
    "                valid_missing_indices = df.index[valid_missing]\n",
    "                if len(valid_missing_indices) > 0:\n",
    "                    valid_missing_x = valid_missing_indices.astype(np.int64) // 10**9\n",
    "                    df.loc[valid_missing, column] = spl(valid_missing_x)\n",
    "\n",
    "        # 计算插值后的空缺值数量\n",
    "        remaining_missing_count = df[column].isna().sum()\n",
    "        filled_count = missing_count - remaining_missing_count\n",
    "\n",
    "        print(f\"插值完成：填充了 {filled_count} 个空缺值，剩余 {remaining_missing_count} 个空缺值\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a26b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 温度插值\n",
    "STR2_tep_data_interpolated = interpolate_temperature_data(\n",
    "    STR2_tep_data, \n",
    "    method='linear',  # 可选：'linear', 'quadratic', 'cubic_spline'\n",
    "    max_gap=2000,             # 允许插值的最大连续空缺值数量\n",
    "    limit_direction='both'  # 双向插值\n",
    ")\n",
    "STR8_tep_data_interpolated = interpolate_temperature_data(\n",
    "    STR2_tep_data, \n",
    "    method='linear',  # 可选：'linear', 'quadratic', 'cubic_spline'\n",
    "    max_gap=2000,             # 允许插值的最大连续空缺值数量\n",
    "    limit_direction='both'  # 双向插值\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14364af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 温度可视化\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for i in np.arange(9)+1:\n",
    "    plt.plot(STR2_tep_data['timestamp'], STR2_tep_data[f'W{i}'], label=f'第{i}层')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('温度')\n",
    "fig.savefig('温度变化图.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e52b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 温度可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in np.arange(9)+1:\n",
    "    plt.plot(STR2_tep_data_interpolated['timestamp'], STR2_tep_data_interpolated[f'W{i}'], label=f'第{i}层')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('温度')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac10023",
   "metadata": {},
   "source": [
    "### 车辙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7769bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 车辙可视化\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(STR2_rut_data['日期'], STR2_rut_data['轮迹带1均值'], color='red', marker='o', label='STR2轮迹带1')\n",
    "plt.plot(STR2_rut_data['日期'], STR2_rut_data['轮迹带2均值'], color='red', marker='^', linestyle='--', label='STR2轮迹带2')\n",
    "plt.plot(STR8_rut_data['日期'], STR8_rut_data['轮迹带1均值'], color='blue', marker='o', label='STR8轮迹带1')\n",
    "plt.plot(STR8_rut_data['日期'], STR8_rut_data['轮迹带2均值'], color='blue', marker='^', linestyle='--', label='STR8轮迹带2')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('车辙深度')\n",
    "fig.savefig('STR2及STR8左右轮迹带车辙.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e0b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_datasets(data1, data2, alpha=0.05, plot=True, savefig=False, fig_name=None):\n",
    "    \"\"\"\n",
    "    比较两列数据是否来自同一总体\n",
    "    \n",
    "    参数:\n",
    "    data1, data2: 待比较的两列数据（Series或numpy数组）\n",
    "    alpha: 显著性水平，默认0.05\n",
    "    plot: 是否绘制可视化图表\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含各项检验结果的字典\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. 正态性检验\n",
    "    stat_shapiro1, p_shapiro1 = stats.shapiro(data1)\n",
    "    stat_shapiro2, p_shapiro2 = stats.shapiro(data2)\n",
    "    results['normality'] = {\n",
    "        'data1': {'statistic': stat_shapiro1, 'p-value': p_shapiro1},\n",
    "        'data2': {'statistic': stat_shapiro2, 'p-value': p_shapiro2}\n",
    "    }\n",
    "    \n",
    "    # 2. t检验（如果两列数据都正态）\n",
    "    if p_shapiro1 > alpha and p_shapiro2 > alpha:\n",
    "        # 方差齐性检验\n",
    "        stat_levene, p_levene = stats.levene(data1, data2)\n",
    "        equal_var = p_levene > alpha\n",
    "        results['levene'] = {'statistic': stat_levene, 'p-value': p_levene, 'equal_var': equal_var}\n",
    "        \n",
    "        # t检验\n",
    "        stat_t, p_t = stats.ttest_ind(data1, data2, equal_var=equal_var)\n",
    "        results['t-test'] = {'statistic': stat_t, 'p-value': p_t}\n",
    "    else:\n",
    "        results['t-test'] = None\n",
    "    \n",
    "    # 3. Mann-Whitney U检验\n",
    "    stat_mw, p_mw = stats.mannwhitneyu(data1, data2)\n",
    "    results['mann-whitney'] = {'statistic': stat_mw, 'p-value': p_mw}\n",
    "    \n",
    "    # 4. Kolmogorov-Smirnov检验\n",
    "    stat_ks, p_ks = stats.ks_2samp(data1, data2)\n",
    "    results['kolmogorov-smirnov'] = {'statistic': stat_ks, 'p-value': p_ks}\n",
    "    \n",
    "    # 5. 可视化\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        \n",
    "        # 直方图\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.histplot(data1, kde=True, label='轮迹带1')\n",
    "        sns.histplot(data2, kde=True, label='轮迹带2')\n",
    "        plt.title('数据分布比较')\n",
    "        plt.xlabel('车辙深度')\n",
    "        plt.ylabel('计数')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 箱线图\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.boxplot(data=[data1, data2])\n",
    "        plt.xticks([0, 1], ['轮迹带1', '轮迹带2'])\n",
    "        plt.title('箱线图比较')\n",
    "        \n",
    "        # Q-Q图\n",
    "        plt.subplot(2, 2, 3)\n",
    "        stats.probplot(data1, dist=\"norm\", plot=plt)\n",
    "        stats.probplot(data2, dist=\"norm\", plot=plt)\n",
    "        plt.title('Q-Q图比较')\n",
    "        plt.xlabel('分位数')\n",
    "        plt.ylabel('车辙深度')\n",
    "        \n",
    "        # 散点图（如果数据长度相同）\n",
    "        if len(data1) == len(data2):\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.scatter(data1, data2, alpha=0.5)\n",
    "            plt.xlabel('轮迹带1')\n",
    "            plt.ylabel('轮迹带2')\n",
    "            plt.title('数据点对比')\n",
    "            # 添加y=x参考线\n",
    "            min_val = min(data1.min(), data2.min())\n",
    "            max_val = max(data1.max(), data2.max())\n",
    "            plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if savefig:\n",
    "            fig.savefig(fig_name, dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # 打印主要结果\n",
    "    print(\"\\n===== 假设检验结果 =====\")\n",
    "    print(f\"1. 正态性检验 (Shapiro-Wilk):\")\n",
    "    print(f\"   轮迹带1: p-value = {p_shapiro1:.4f} ({'正态' if p_shapiro1 > alpha else '非正态'})\")\n",
    "    print(f\"   轮迹带2: p-value = {p_shapiro2:.4f} ({'正态' if p_shapiro2 > alpha else '非正态'})\")\n",
    "    \n",
    "    if results['t-test']:\n",
    "        print(f\"\\n2. t检验:\")\n",
    "        print(f\"   方差齐性: p-value = {p_levene:.4f} ({'满足' if equal_var else '不满足'})\")\n",
    "        print(f\"   t统计量 = {stat_t:.4f}, p-value = {p_t:.4f} ({'相同分布' if p_t > alpha else '不同分布'})\")\n",
    "    \n",
    "    print(f\"\\n3. Mann-Whitney U检验:\")\n",
    "    print(f\"   U统计量 = {stat_mw:.4f}, p-value = {p_mw:.4f} ({'相同分布' if p_mw > alpha else '不同分布'})\")\n",
    "    \n",
    "    print(f\"\\n4. Kolmogorov-Smirnov检验:\")\n",
    "    print(f\"   KS统计量 = {stat_ks:.4f}, p-value = {p_ks:.4f} ({'相同分布' if p_ks > alpha else '不同分布'})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c421bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_result = compare_two_datasets(STR2_rut_data['轮迹带1均值'],\n",
    "                                   STR8_rut_data['轮迹带1均值'],\n",
    "                                   alpha=0.05, plot=True, savefig=False, \n",
    "                                   fig_name='STR2及STR8右轮迹带分析.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18144930",
   "metadata": {},
   "source": [
    "### 应力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43127e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 查看\n",
    "# 对应关系：STR2压力传感器的Y1对应温度传感器的W2...压力传感器的Y7对应温度传感器的W8\n",
    "# \n",
    "print(STR2_stress_data.head())\n",
    "print(STR2_tep_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45126c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 应力可视化\n",
    "fig = plt.figure(figsize=(10, 50))\n",
    "for i in np.arange(7)+1:\n",
    "    plt.subplot(7, 1, i)\n",
    "    plt.hist(STR2_stress_data[f'Y{i}'], bins=100, density=True, label=f'第{i}层')\n",
    "    plt.xlabel('应力')\n",
    "    plt.ylabel('密度')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e73823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应力可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in np.arange(7)+1:\n",
    "    plt.plot(STR2_stress_data['timestamp'], STR2_stress_data[f'Y{i}'], label=f'第{i}层')\n",
    "# plt.plot(STR2_stress_data['timestamp'], STR2_stress_data['S1'], label='第1层')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('应力')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应力可视化\n",
    "# 抽一段来看\n",
    "# 计算时间范围\n",
    "sample_start_time = pd.Timestamp('2020-03-21 10:10:00')\n",
    "sample_end_time = pd.Timestamp('2020-03-21 10:20:00')\n",
    "\n",
    "sample_stress = STR2_stress_data[(STR2_stress_data['timestamp'] >= sample_start_time) & (STR2_stress_data['timestamp'] <= sample_end_time)]\n",
    "sample_stress.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应力可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in np.arange(7)+1:\n",
    "    plt.plot(sample_stress['timestamp'], sample_stress[f'Y{i}'], label=f'第{i}层')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('应力')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_clustering(df, n_clusters=2, random_state=42, plot=True, get_score=False):\n",
    "    \"\"\"\n",
    "    对传感器数据进行KMeans聚类，区分正常状态和响应状态\n",
    "    \n",
    "    参数:\n",
    "    df (pd.DataFrame): 包含传感器数据的DataFrame，第一列为日期，其余列为传感器数据(S1~Sn)\n",
    "    n_clusters (int): 聚类数量，默认为2\n",
    "    random_state (int): 随机种子，保证结果可复现\n",
    "    plot (bool): 是否绘制聚类结果可视化图\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含聚类结果的字典，包括：\n",
    "        - cluster_centers: 聚类中心\n",
    "        - labels: 每个样本的聚类标签\n",
    "        - df_clustered: 包含聚类标签的原始DataFrame\n",
    "        - inertia: 聚类的惯性（越小表示聚类效果越好）\n",
    "    \"\"\"\n",
    "    # 提取特征列（跳过第一列日期）\n",
    "    feature_columns = df.columns[1:]\n",
    "    X = df[feature_columns].values\n",
    "    \n",
    "    # 数据标准化（KMeans对尺度敏感）\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 执行KMeans聚类\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    # 评价聚类效果\n",
    "    if get_score:\n",
    "        score = silhouette_score(X, labels)\n",
    "        print(f\"轮廓系数：{score:.2f}\")\n",
    "    \n",
    "    # 获取聚类中心并反标准化\n",
    "    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    \n",
    "    # 将聚类结果添加到原始DataFrame\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['Cluster'] = labels\n",
    "    \n",
    "    # 为聚类标签排序（假设较小的值对应A状态，较大的值对应B状态）\n",
    "    # 计算每个聚类中心的平均传感器值\n",
    "    cluster_avg = np.mean(cluster_centers, axis=1)\n",
    "    # 获取排序后的索引\n",
    "    sorted_indices = np.argsort(cluster_avg)\n",
    "    # 创建映射字典\n",
    "    label_mapping = {sorted_indices[i]: i for i in range(n_clusters)}\n",
    "    # 重新映射标签\n",
    "    df_clustered['Cluster'] = df_clustered['Cluster'].map(label_mapping)\n",
    "    \n",
    "    # 更新聚类中心顺序\n",
    "    cluster_centers = cluster_centers[sorted_indices]\n",
    "    \n",
    "    # 绘制聚类结果\n",
    "    if plot:\n",
    "        plot_clustering_results(df_clustered, feature_columns, cluster_centers)\n",
    "    \n",
    "    return {\n",
    "        'cluster_centers': cluster_centers,\n",
    "        'labels': df_clustered['Cluster'].values,\n",
    "        'df_clustered': df_clustered,\n",
    "        'inertia': inertia\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a66bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_results(df, feature_columns, cluster_centers):\n",
    "    \"\"\"绘制聚类结果可视化图\"\"\"\n",
    "    n_features = len(feature_columns)\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(12, 4 * n_features))\n",
    "    if n_features == 1:\n",
    "        axes = [axes]  # 确保axes是列表\n",
    "    \n",
    "    # 为每个传感器绘制聚类结果\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # 绘制原始数据\n",
    "        ax.plot(df.index, df[col], 'b-', alpha=0.5, label='原始数据')\n",
    "        \n",
    "        # 为不同聚类绘制不同颜色\n",
    "        for cluster in df['Cluster'].unique():\n",
    "            cluster_data = df[df['Cluster'] == cluster]\n",
    "            ax.scatter(cluster_data.index, cluster_data[col], \n",
    "                       label=f'聚类 {cluster} (中心: {cluster_centers[cluster][i]:.2f})',\n",
    "                       alpha=0.6, s=30)\n",
    "        \n",
    "        ax.set_title(f'{col} 的聚类结果')\n",
    "        ax.set_xlabel('日期')\n",
    "        ax.set_ylabel('传感器值')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 绘制聚类中心热图\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    centers_df = pd.DataFrame(cluster_centers, columns=feature_columns, \n",
    "                              index=[f'聚类 {i}' for i in range(len(cluster_centers))])\n",
    "    sns.heatmap(centers_df, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('聚类中心值')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensor_pairplot(df, cluster_column='Cluster', figsize=(20, 20)):\n",
    "    \"\"\"\n",
    "    绘制传感器数据的散点图矩阵，展示各传感器之间的关系\n",
    "    \n",
    "    参数:\n",
    "    df (pd.DataFrame): 包含传感器数据的DataFrame，第一列为日期，其余列为传感器数据(S1~Sn)\n",
    "    cluster_column (str): 聚类标签列名，用于为散点着色\n",
    "    figsize (tuple): 图形大小，单位为英寸\n",
    "    \n",
    "    返回:\n",
    "    plt.Figure: 生成的图形对象\n",
    "    \"\"\"\n",
    "    # 提取传感器列（跳过第一列日期）\n",
    "    sensor_columns = df.columns[1:-1]\n",
    "    \n",
    "    # 创建图形和子图网格\n",
    "    n = len(sensor_columns)\n",
    "    fig, axes = plt.subplots(n, n, figsize=figsize)\n",
    "    \n",
    "    # 确保axes是二维数组（即使n=1）\n",
    "    axes = np.array(axes).reshape(n, n)\n",
    "    \n",
    "    # 设置中文字体\n",
    "    plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "    \n",
    "    # 遍历每个子图\n",
    "    for i, row_col in enumerate(sensor_columns):\n",
    "        for j, col_col in enumerate(sensor_columns):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # 如果是对角线位置，绘制直方图\n",
    "            if i == j:\n",
    "                for cluster in df[cluster_column].unique():\n",
    "                    sns.histplot(\n",
    "                        df[df[cluster_column] == cluster][row_col],\n",
    "                        ax=ax,\n",
    "                        kde=True,\n",
    "                        label=f'聚类 {cluster}',\n",
    "                        alpha=0.6\n",
    "                    )\n",
    "                ax.set_title(f'{row_col} 分布')\n",
    "                ax.legend()\n",
    "            \n",
    "            # 非对角线位置，绘制散点图\n",
    "            else:\n",
    "                sns.scatterplot(\n",
    "                    x=col_col,\n",
    "                    y=row_col,\n",
    "                    hue=cluster_column,\n",
    "                    data=df,\n",
    "                    ax=ax,\n",
    "                    alpha=0.6,\n",
    "                    palette='Set1'\n",
    "                )\n",
    "                ax.set_title(f'{row_col} vs {col_col}')\n",
    "            \n",
    "            # 简化坐标轴标签\n",
    "            if i < n - 1:\n",
    "                ax.set_xlabel('')\n",
    "            if j > 0:\n",
    "                ax.set_ylabel('')\n",
    "    \n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tep_and_stress(tep_df, stress_df, sensor_name):\n",
    "    \"\"\"\n",
    "    tep_df和stress_df都只有两列\n",
    "    对于tep_df的每一行，找到这段时间范围内最大的应力.\n",
    "    sensor_name是'Y1'这样的\n",
    "    \"\"\"\n",
    "    start_date = min(stress_df['timestamp'])\n",
    "    end_date = max(stress_df['timestamp'])\n",
    "    tep_df = tep_df[(tep_df['timestamp'] >= start_date) & (tep_df['timestamp'] <= end_date)]\n",
    "    tep_df = tep_df.reset_index(drop=True)\n",
    "    tep_df.loc[:, sensor_name] = None\n",
    "    \n",
    "    time_delta = timedelta(minutes=10)\n",
    "    for i in range(len(tep_df)):\n",
    "        start_time = tep_df.iloc[i, 0]\n",
    "        end_time = start_time + time_delta\n",
    "        stress_df_tmp = stress_df[(stress_df['timestamp'] >= start_time) & (stress_df['timestamp'] <= end_time)]\n",
    "        if len(stress_df_tmp) > 0:\n",
    "            max_stress = max(stress_df_tmp.loc[:, sensor_name])\n",
    "            tep_df.loc[i, sensor_name] = max_stress\n",
    "    \n",
    "    return tep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nearest_to_mean(df, temp_col='W2', value_col='Y1', n_bins=5, m=3):\n",
    "    \"\"\"\n",
    "    将温度列分组，每组选取最接近均值的m条数据\n",
    "    \n",
    "    参数:\n",
    "    df: 输入DataFrame\n",
    "    temp_col: 温度列名\n",
    "    value_col: 传感器数值列名\n",
    "    n_bins: 温度分组数\n",
    "    m: 每组选取的数据条数\n",
    "    \n",
    "    返回:\n",
    "    筛选后的DataFrame\n",
    "    \"\"\"\n",
    "    # 步骤1: 创建温度分箱\n",
    "    df['temp_bin'] = pd.cut(df[temp_col], bins=n_bins)\n",
    "    \n",
    "    # 步骤2: 计算每个箱内的均值\n",
    "    bin_means = df.groupby('temp_bin')[value_col].mean().reset_index()\n",
    "    bin_means.columns = ['temp_bin', 'bin_mean']\n",
    "    \n",
    "    # 步骤3: 将均值合并回原DataFrame\n",
    "    df = pd.merge(df, bin_means, on='temp_bin', how='left')\n",
    "    \n",
    "    # 步骤4: 计算每个值与均值的绝对差\n",
    "    df['abs_diff'] = (df[value_col] - df['bin_mean']).abs()\n",
    "    \n",
    "    # 步骤5: 对每个箱内的数据按绝对差排序，并选择前m条\n",
    "    selected = df.sort_values(['temp_bin', 'abs_diff']).groupby('temp_bin').head(m)\n",
    "    \n",
    "    # 步骤6: 删除临时列并返回结果\n",
    "    return selected.drop(['temp_bin', 'bin_mean', 'abs_diff'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6955d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nonlinear_model(df, x_col='tep', y_col='stress', plot=True):\n",
    "    \"\"\"\n",
    "    拟合非线性模型，并评价拟合效果\n",
    "    \n",
    "    参数:\n",
    "    df: DataFrame，包含tep和stress两列\n",
    "    x_col, y_col: 自变量和因变量的列名\n",
    "    plot: 是否绘制拟合结果和残差图\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含拟合参数和评价指标的字典\n",
    "    \"\"\"\n",
    "    # 提取数据\n",
    "    x_data = df[x_col].values\n",
    "    y_data = df[y_col].values\n",
    "    \n",
    "    # 定义新模型\n",
    "    def model(tep, a, b):\n",
    "        return a * (b ** tep)\n",
    "    \n",
    "    # 初始参数猜测（关键！影响拟合成功率）\n",
    "    p0 = [np.mean(y_data), 0.9]  # 初始猜测值，需根据数据调整\n",
    "    \n",
    "    # 拟合模型（增加参数边界约束）\n",
    "    try:\n",
    "        popt, pcov = curve_fit(\n",
    "            model, \n",
    "            x_data, \n",
    "            y_data, \n",
    "            p0=p0,\n",
    "            bounds=([0, 0], [np.inf, np.inf])\n",
    "        )\n",
    "        a_fit, b_fit = popt\n",
    "        \n",
    "        # 计算拟合值\n",
    "        y_fit = model(x_data, *popt)\n",
    "        \n",
    "        # 评价指标\n",
    "        r2 = r2_score(y_data, y_fit)\n",
    "        rmse = np.sqrt(mean_squared_error(y_data, y_fit))\n",
    "        \n",
    "        # 残差分析\n",
    "        residuals = y_data - y_fit\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"拟合参数: a = {a_fit:.4f}, b = {b_fit:.4f}\")\n",
    "        print(f\"评价指标: R² = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
    "        \n",
    "        # 绘图\n",
    "        if plot:\n",
    "            fig = plt.figure(figsize=(10, 10))\n",
    "            plt.scatter(x_data, y_data, label='原始数据', alpha=0.6)\n",
    "            plt.plot(x_data, y_fit, 'r-', \n",
    "                     label=f'拟合曲线: {y_col} = {a_fit:.4f} * {b_fit:.4f}^{x_col}', \n",
    "                     linewidth=2)\n",
    "            plt.xlabel(x_col)\n",
    "            plt.ylabel(y_col)\n",
    "            plt.title('非线性拟合结果')\n",
    "            plt.legend()\n",
    "            fig.savefig('图片/应力对应关系')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "        return {\n",
    "            'params': {'a': a_fit, 'b': b_fit},\n",
    "            'metrics': {'r2': r2, 'rmse': rmse},\n",
    "            'residuals': residuals\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"拟合失败: {e}\")\n",
    "        print(\"提示: 尝试调整初始参数p0或检查数据范围\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fcb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始聚类\n",
    "kmeans_result = sensor_clustering(STR2_stress_data, n_clusters=2, random_state=42, plot=False, get_score=False)\n",
    "STR2_stress_clustered = kmeans_result['df_clustered']\n",
    "STR2_stress_responsive = STR2_stress_clustered[STR2_stress_clustered['Cluster'] == 1]  # 只保留响应值\n",
    "# fig =  plot_sensor_pairplot(STR2_stress_responsive)\n",
    "# fig.savefig('STR2传感器聚类结果', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始融合\n",
    "merge_df = merge_tep_and_stress(STR2_tep_data[['timestamp', 'W2']], \n",
    "                                STR2_stress_responsive[['timestamp', 'Y1']], 'Y1')\n",
    "merge_df = merge_df.dropna(subset=['Y1'])\n",
    "print(len(merge_df))\n",
    "print(merge_df.head())\n",
    "clear_merge_df = select_nearest_to_mean(merge_df, n_bins=120, m=2)\n",
    "# merge_df.to_csv('111.csv', index=False)  # index=False 表示不保存索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始拟合\n",
    "fit_results = fit_nonlinear_model(clear_merge_df, x_col='W2', y_col='Y1', plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907bb51",
   "metadata": {},
   "source": [
    "# 车辙预估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafbeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(N, T, RD, window_length=5, polyorder=2):\n",
    "    \"\"\"数据预处理：处理NaN、负数和零值，并对RD进行平滑化处理\"\"\"\n",
    "    # 转换为numpy数组\n",
    "    N = np.array(N, dtype=np.float64)\n",
    "    T = np.array(T, dtype=np.float64)\n",
    "    RD = np.array(RD, dtype=np.float64)\n",
    "    \n",
    "    # 处理NaN（用前值填充）\n",
    "    N = pd.Series(N).fillna(method='ffill').values\n",
    "    T = pd.Series(T).fillna(method='ffill').values\n",
    "    RD = pd.Series(RD).fillna(method='ffill').values\n",
    "    \n",
    "    # 处理非正数轴次（替换为极小值）\n",
    "    N[N <= 0] = 1e-6\n",
    "    \n",
    "    # 处理负数温度（平移至正数区间）\n",
    "    T_min = np.min(T)\n",
    "    if T_min < 0:\n",
    "        T = T - T_min + 1e-6\n",
    "    \n",
    "    # 处理非正数车辙深度（取绝对值，需谨慎评估物理意义）\n",
    "    RD = np.abs(RD)\n",
    "    \n",
    "    # 对RD进行平滑化处理（使用Savitzky-Golay滤波器）\n",
    "    # 仅在数据点足够多时进行平滑处理\n",
    "    if len(RD) > window_length:\n",
    "        RD_smooth = savgol_filter(RD, window_length=window_length, polyorder=polyorder)\n",
    "        \n",
    "        # 确保平滑后的值不小于零\n",
    "        RD_smooth = np.maximum(RD_smooth, 1e-10)\n",
    "        \n",
    "        # 可视化平滑效果（可选，取消注释以查看）\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(RD, 'b-', label='原始RD')\n",
    "        plt.plot(RD_smooth, 'r--', label='平滑后RD')\n",
    "        plt.legend()\n",
    "        plt.title('车辙深度平滑处理效果')\n",
    "        plt.show()\n",
    "        \n",
    "        RD = RD_smooth\n",
    "    \n",
    "    return N, T, RD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c193cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearized_objective(beta, N, T, RD):\n",
    "    \"\"\"对数线性化模型的目标函数\"\"\"\n",
    "    beta0, n, t, dt = beta\n",
    "    months = len(N)\n",
    "    log_rd_pred = np.zeros(months)\n",
    "    \n",
    "    for i in range(months):\n",
    "        \n",
    "        if i == 0:\n",
    "            log_rd_pred[i] = beta0 + n * np.log(N[i]) + t * np.log((T[i] + dt))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # 计算当前月的累计等效轴次\n",
    "            nt = np.exp((log_rd_pred[i-1] - beta0 - t * np.log(T[i] + dt)) / n) + N[i]\n",
    "\n",
    "            # 计算对数预测值\n",
    "            log_rd_pred[i] = beta0 + n * np.log(nt) + t * np.log(T[i] + dt)\n",
    "    \n",
    "    # 计算对数空间的均方误差\n",
    "    mse_log = mean_squared_error(np.log(RD), log_rd_pred)\n",
    "    return mse_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linearized_model(N, T, RD, initial_guess):\n",
    "    \"\"\"使用对数线性化方法训练模型\"\"\"\n",
    "    \n",
    "    # 定义参数约束\n",
    "    # beta0无约束，n和t必须为正数\n",
    "    bounds = [(None, None), (1e-10, None), (1e-10, None), (None, None)]\n",
    "    \n",
    "    # 使用L-BFGS-B算法进行优化\n",
    "    result = minimize(\n",
    "        linearized_objective,\n",
    "        initial_guess,\n",
    "        args=(N, T, RD),\n",
    "        method='L-BFGS-B',\n",
    "        bounds=bounds,\n",
    "        options={'maxiter': 1000, 'disp': True}\n",
    "    )\n",
    "    \n",
    "    # 返回最优参数\n",
    "    beta0_opt, n_opt, t_opt, dt_opt = result.x\n",
    "    A_opt = np.exp(beta0_opt)  # 转换回原始参数A\n",
    "    \n",
    "    return A_opt, n_opt, t_opt, dt_opt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rd_linearized(params, N, T):\n",
    "    \"\"\"使用对数线性化模型预测车辙深度\"\"\"\n",
    "    A, n, t, dt = params\n",
    "    months = len(N)\n",
    "    rd_pred = np.zeros(months)\n",
    "    \n",
    "    for i in range(months):\n",
    "        if i == 0:\n",
    "            # 第一个月直接计算\n",
    "            rd_pred[i] = A * (N[i] ** n) * ((T[i] + dt) ** t)\n",
    "        else:\n",
    "            # 计算前一个月的等效轴次\n",
    "            equivalent_axles_prev = (rd_pred[i-1] / (A * ((T[i-1] + dt) ** t))) ** (1/n)\n",
    "            # 计算累计等效轴次\n",
    "            total_equivalent_axles = equivalent_axles_prev + N[i]\n",
    "            # 计算当前月的车辙深度\n",
    "            rd_pred[i] = A * (total_equivalent_axles ** n) * ((T[i] + dt) ** t)\n",
    "    \n",
    "    return rd_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(params, N, T, RD_actual):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    rd_pred = predict_rd_linearized(params, N, T)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = mean_squared_error(RD_actual, rd_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(RD_actual, rd_pred)\n",
    "    \n",
    "    # 计算平均绝对百分比误差(MAPE)\n",
    "    mape = np.mean(np.abs((RD_actual - rd_pred) / RD_actual)) * 100\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41fedea",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19540987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(axle_data, rut_data, tep_data, layer_name):\n",
    "    # 将车辙数据、轴次数据和温度数据同步到同一个表格中\n",
    "    # layer_name指W1,W2这些\n",
    "    # STR2车辙深度主要由第一层决定\n",
    "    # STR8车辙深度主要由第前三层决定，分别是0.8，0.2，0.2\n",
    "    rd_data = pd.DataFrame(columns=['日期', '断面均值', '周期内轴次', layer_name])\n",
    "    rd_data['日期'] = rut_data['日期']\n",
    "    rd_data['断面均值'] = rut_data['断面均值']\n",
    "    \n",
    "    # 计算周期内轴次和平均温度\n",
    "    start_date = axle_data.iloc[0, 0]\n",
    "    for i in range(len(rd_data)):\n",
    "        end_date = rd_data.iloc[i, 0]\n",
    "        axle_data_tmp = axle_data[(axle_data['日期'] >= start_date) & (axle_data['日期'] <= end_date)]\n",
    "        tep_data_tmp = tep_data[(tep_data['timestamp'] >= start_date) & (tep_data['timestamp'] <= end_date)]\n",
    "        rd_data.loc[i, '周期内轴次'] = axle_data_tmp['边缘轴次'].sum()\n",
    "        rd_data.loc[i, layer_name] = tep_data_tmp[layer_name].mean()\n",
    "        start_date = end_date + timedelta(days=1)\n",
    "    \n",
    "    return rd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418728f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'W1'\n",
    "rd_data = prepare_data(axle_data, STR2_rut_data, STR2_tep_data, layer_name)\n",
    "N = np.array(rd_data['周期内轴次'])\n",
    "T = np.array(rd_data[layer_name])\n",
    "RD_actual = np.array(rd_data['断面均值'])\n",
    "initial_guess = [-25, 0.5, 5, 0]\n",
    "print(rd_data)\n",
    "print(len(rd_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd4a78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "months = len(rd_data)\n",
    "\n",
    "# 数据预处理\n",
    "N, T, RD_actual = preprocess_data(N, T, RD_actual)\n",
    "\n",
    "# 训练模型\n",
    "A_opt, n_opt, t_opt, dt_opt, opt_result = train_linearized_model(N, T, RD_actual, initial_guess)\n",
    "optimal_params = (A_opt, n_opt, t_opt, dt_opt)\n",
    "\n",
    "print(f\"\\n优化后的参数: A = {A_opt:.6f}, n = {n_opt:.6f}, t = {t_opt:.6f}, dt = {dt_opt:.6f}\")\n",
    "\n",
    "# 评估模型\n",
    "metrics = evaluate_model(optimal_params, N, T, RD_actual)\n",
    "print(\"\\n模型评估结果:\")\n",
    "print(f\"MSE: {metrics['mse']:.6f}\")\n",
    "print(f\"RMSE: {metrics['rmse']:.6f}\")\n",
    "print(f\"R²: {metrics['r2']:.4f}\")\n",
    "print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "RD_pred = predict_rd_linearized(optimal_params, N, T)\n",
    "\n",
    "# 可视化结果\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 绘制预测值与实际值对比\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(1, months + 1), RD_actual, 'b-', label='实际车辙深度')\n",
    "plt.plot(range(1, months + 1), RD_pred, 'r--', label='预测车辙深度')\n",
    "plt.xlabel('月份')\n",
    "plt.ylabel('车辙深度 (mm)')\n",
    "plt.title('车辙深度预测模型结果')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 绘制残差图\n",
    "plt.subplot(2, 1, 2)\n",
    "residuals = RD_actual - RD_pred\n",
    "plt.scatter(range(1, months + 1), residuals, color='gray')\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('月份')\n",
    "plt.ylabel('残差 (mm)')\n",
    "plt.title('残差分析')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 输出预测与实际值的对比表格\n",
    "results_df = pd.DataFrame({\n",
    "    '日期': rd_data['日期'],\n",
    "    '温度 (°C)': T.round(1),\n",
    "    '轴次': N.round(1),\n",
    "    '实际车辙深度': RD_actual.round(4),\n",
    "    '预测车辙深度': RD_pred.round(4),\n",
    "    '误差': (RD_actual - RD_pred).round(4),\n",
    "    '误差百分比 (%)': (residuals / RD_actual * 100).round(2)\n",
    "})\n",
    "\n",
    "# print(\"\\n预测结果与实际值对比:\")\n",
    "# print(results_df.to_string(index=False))\n",
    "# results_df.to_csv('pred_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c55f6",
   "metadata": {},
   "source": [
    "### STL分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_data(df, freq='QS'):\n",
    "    \"\"\"\n",
    "    处理稀疏时间序列数据（每月1-4条记录）\n",
    "    \n",
    "    参数:\n",
    "    df (pd.DataFrame): 包含时间戳和数值的DataFrame\n",
    "    freq (str): 重采样频率，建议使用'QS'（季度）或'6MS'（半年）\n",
    "    \n",
    "    返回:\n",
    "    pd.Series: 处理后的连续时间序列\n",
    "    \"\"\"\n",
    "    # 设置时间戳为索引\n",
    "    df = df.set_index(df.columns[0])\n",
    "    \n",
    "    # 按指定频率重采样\n",
    "    resampled = df.resample(freq).mean()\n",
    "    \n",
    "    # 使用三次样条插值（更适合稀疏数据）\n",
    "    # 注意：样条插值要求至少有4个非NaN值\n",
    "    if resampled.count().iloc[0] >= 4:\n",
    "        interpolated = resampled.interpolate(method='spline', order=3)\n",
    "    else:\n",
    "        # 如果数据点太少，退化为线性插值\n",
    "        print(\"警告：数据点过少，使用线性插值而非样条插值\")\n",
    "        interpolated = resampled.interpolate(method='time')\n",
    "    \n",
    "    return interpolated.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_stl_decomposition(series, period=12):\n",
    "    \"\"\"执行STL分解（与之前相同）\"\"\"\n",
    "    stl = STL(series, period=period)\n",
    "    result = stl.fit()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        '观测值': series,\n",
    "        '趋势': result.trend,\n",
    "        '季节': result.seasonal,\n",
    "        '残差': result.resid\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化函数（与之前相同）\n",
    "def plot_decomposition(decomposition_results):\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12, 15), sharex=True)\n",
    "    for i, col in enumerate(decomposition_results.columns):\n",
    "        axes[i].plot(decomposition_results[col], marker='o')\n",
    "        axes[i].set_title(col)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stl_main(df, freq='QS', period=12):\n",
    "    \"\"\"\n",
    "    处理稀疏时间序列并执行STL分解\n",
    "    \n",
    "    参数:\n",
    "    df (pd.DataFrame): 输入数据\n",
    "    freq (str): 重采样频率，建议'QS'（季度）\n",
    "    period (int): 分解周期，建议12（4年周期）\n",
    "    \n",
    "    返回:\n",
    "    tuple: 分解结果和图表\n",
    "    \"\"\"\n",
    "    # 准备稀疏数据\n",
    "    prepared_series = prepare_sparse_data(df, freq=freq)\n",
    "    \n",
    "    # 执行STL分解\n",
    "    decomposition = perform_stl_decomposition(prepared_series, period=period)\n",
    "    \n",
    "    # 可视化\n",
    "    fig = plot_decomposition(decomposition)\n",
    "    \n",
    "    return decomposition, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a73cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始STL分解\n",
    "stl_data = results_df[['日期', '预测车辙深度']]\n",
    "decomposition, fig = stl_main(stl_data, freq='QS', period=12)\n",
    "# fig.savefig('STR8_STL分解.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef06a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_timestamps(original_df, decomposition_results, decay, freq='QS'):\n",
    "    \"\"\"\n",
    "    根据STL分解结果，重构原始时间戳对应的值\n",
    "    \n",
    "    参数:\n",
    "    original_df (pd.DataFrame): 原始数据，包含时间戳和观测值\n",
    "    decomposition_results (pd.DataFrame): STL分解结果\n",
    "    freq (str): 分解时使用的频率\n",
    "    \n",
    "    返回:\n",
    "    pd.DataFrame: 包含原始时间戳和重构值的DataFrame\n",
    "    \"\"\"\n",
    "    # 提取分解后的成分\n",
    "    trend = decomposition_results['趋势']\n",
    "    seasonal = decomposition_results['季节']\n",
    "    residual = decomposition_results['残差']\n",
    "    \n",
    "    # 获取原始时间戳并转换为Index对象\n",
    "    original_timestamps = pd.Index(original_df.iloc[:, 0])  # 转换为Index对象\n",
    "    \n",
    "    # 将分解结果重采样到原始时间戳\n",
    "    # 1. 首先将分解结果转换为DataFrame\n",
    "    components = pd.DataFrame({\n",
    "        '趋势': trend,\n",
    "        '季节': seasonal,\n",
    "        '残差': residual\n",
    "    })\n",
    "    \n",
    "    # 2. 将各成分重采样到原始时间戳\n",
    "    reconstructed = pd.DataFrame(index=original_timestamps)\n",
    "    \n",
    "    for component in ['趋势', '季节', '残差']:\n",
    "        # 使用时间插值将规则序列转换到原始时间戳\n",
    "        reconstructed[component] = components[component].reindex(\n",
    "            original_timestamps.union(components.index)\n",
    "        ).interpolate(method='time').reindex(original_timestamps)\n",
    "    \n",
    "    # 2.5 压缩\n",
    "    decay_arr = np.exp(-1 * decay * np.arange(len(reconstructed)))\n",
    "    \n",
    "    # 3. 合成预测值\n",
    "    reconstructed['合成值'] = reconstructed['趋势'] + decay_arr * reconstructed['季节']\n",
    "    \n",
    "    # 4. 添加原始观测值以便比较\n",
    "    reconstructed['原始观测值'] = original_df.iloc[:, 1].values  # 假设第二列是观测值\n",
    "    \n",
    "    # 5. 计算误差\n",
    "    reconstructed['误差'] = reconstructed['合成值'] - reconstructed['原始观测值']\n",
    "    reconstructed['相对误差(%)'] = (reconstructed['误差'] / reconstructed['原始观测值']) * 100\n",
    "    \n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(reconstructed_df):\n",
    "    \"\"\"绘制重构结果与原始数据的对比图\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制原始观测值和合成值\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(reconstructed_df.index, reconstructed_df['原始观测值'], 'o-', label='原始观测值')\n",
    "    plt.plot(reconstructed_df.index, reconstructed_df['合成值'], 's-', label='合成值')\n",
    "    plt.legend()\n",
    "    plt.title('原始观测值 vs 合成值')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制误差\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(reconstructed_df.index, reconstructed_df['误差'], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title('重构误差')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0faf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_main(original_df, decomposition, decay, freq='QS'):\n",
    "    \"\"\"\n",
    "    主函数：重构原始时间戳的值并可视化结果\n",
    "    \n",
    "    参数:\n",
    "    original_df (pd.DataFrame): 原始数据\n",
    "    decomposition (pd.DataFrame): STL分解结果\n",
    "    freq (str): 重采样频率\n",
    "    \n",
    "    返回:\n",
    "    tuple: 重构结果和图表\n",
    "    \"\"\"\n",
    "    # 重构原始时间戳的值\n",
    "    reconstructed = reconstruct_original_timestamps(original_df, decomposition, decay, freq=freq)\n",
    "    \n",
    "    # 可视化结果\n",
    "    plot = plot_reconstruction(reconstructed)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    mse = ((reconstructed['误差']) ** 2).mean()\n",
    "    mae = abs(reconstructed['误差']).mean()\n",
    "    mape = abs(reconstructed['相对误差(%)']).mean()\n",
    "    \n",
    "    print(f\"重构评估指标:\")\n",
    "    print(f\"均方误差 (MSE): {mse:.4f}\")\n",
    "    print(f\"平均绝对误差 (MAE): {mae:.4f}\")\n",
    "    print(f\"平均绝对百分比误差 (MAPE): {mape:.2f}%\")\n",
    "    \n",
    "    return reconstructed, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始重建\n",
    "reconstructed, plot = reconstruction_main(stl_data, decomposition, decay=0.075, freq='QS')\n",
    "reconstructed['真实值'] = np.array(results_df['实际车辙深度'])\n",
    "plt.scatter(reconstructed.index, reconstructed['真实值'], c='r')\n",
    "plt.plot(reconstructed.index, reconstructed['合成值'], c='b', linewidth=3)\n",
    "plt.show()\n",
    "r2 = r2_score(reconstructed['真实值'], reconstructed['合成值'])\n",
    "print(r2)\n",
    "print(reconstructed.head())\n",
    "reconstructed.to_csv('结果数据/STR8结果.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77432b36",
   "metadata": {},
   "source": [
    "### 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949f332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "STR2_result = pd.read_csv('结果数据/STR2结果.csv', parse_dates=['日期'])\n",
    "STR8_result = pd.read_csv('结果数据/STR8结果.csv', parse_dates=['日期'])\n",
    "print(STR2_result.head())\n",
    "print(STR8_result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 9))\n",
    "plt.scatter(STR2_result['日期'], STR2_result['真实值'], marker='^', c='b', label='STR2真实车辙')\n",
    "plt.plot(STR2_result['日期'], STR2_result['合成值'], linewidth=3, c='b', label='STR2预测车辙')\n",
    "plt.plot(STR2_result['日期'], STR2_result['原始观测值'], linewidth=1, linestyle='--', c='b', label='STR2预测车辙（未重新合成）')\n",
    "plt.scatter(STR8_result['日期'], STR8_result['真实值'], marker='^', c='g', label='STR8真实车辙')\n",
    "plt.plot(STR2_result['日期'], STR8_result['合成值'], linewidth=3, c='g', label='STR8预测车辙')\n",
    "plt.plot(STR8_result['日期'], STR8_result['原始观测值'], linewidth=1, linestyle='--', c='g', label='STR8预测车辙（未重新合成）')\n",
    "plt.legend()\n",
    "plt.xlabel('日期')\n",
    "plt.ylabel('车辙深度')\n",
    "plt.title('车辙长期累积模型')\n",
    "plt.grid(True)\n",
    "# fig.savefig('图片/预测数据.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "max_value1 = np.max([np.max(STR2_result['真实值']), np.max(STR2_result['合成值'])])\n",
    "max_value2 = np.max([np.max(STR8_result['真实值']), np.max(STR8_result['合成值'])])\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(STR2_result['真实值'], STR2_result['合成值'], marker='o', c='b')\n",
    "plt.plot([10, max_value1], [10, max_value1], c='r', linestyle='--')\n",
    "plt.title('STR2预测结果')\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(STR8_result['真实值'], STR8_result['合成值'], marker='o', c='b')\n",
    "plt.plot([10, max_value2], [10, max_value2], c='r', linestyle='--')\n",
    "plt.title('STR8预测结果')\n",
    "# fig.savefig('图片/预测数据2.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
